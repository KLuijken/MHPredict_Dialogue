(window.webpackJsonp=window.webpackJsonp||[]).push([[0],{14:function(e,t,a){},15:function(e,t,a){},16:function(e,t,a){"use strict";a.r(t);var r=a(0),n=a.n(r),i=a(7),l=a.n(i),o=(a(14),a(1)),s=a(2),c=a(4),d=a(3),m=a(5),u=function(e){function t(){var e;return Object(o.a)(this,t),(e=Object(c.a)(this,Object(d.a)(t).call(this))).state={exp_visible:!1},e}return Object(m.a)(t,e),Object(s.a)(t,[{key:"handleClick",value:function(e){this.setState({exp_visible:!this.state.exp_visible})}},{key:"componentDidUpdate",value:function(e,t){t===this.state&&this.setState({exp_visible:!1})}},{key:"render",value:function(){var e=this.props,t=this.state,a=e.children.filter?e.children.filter(function(e){return"exp"===e.type}):"",r=e.children.filter?e.children.filter(function(e){return"exp"!==e.type}):e.children;return n.a.createElement("div",Object.assign({},e,{className:(e.isB2?"speech-bubble2 ":"speech-bubble1 ")+e.className}),n.a.createElement("div",{onClick:this.handleClick.bind(this)},r),n.a.createElement("div",{className:"ackshually",style:{color:"#606060",display:t.exp_visible?"block":"none"}},a))}}]),t}(r.Component),h=function(e){return n.a.createElement("div",null,n.a.createElement("span",{className:"user"},e.user,"  "),n.a.createElement(u,Object.assign({},e,{isB2:!0}),e.children))},p=(a(15),function(e){function t(){return Object(o.a)(this,t),Object(c.a)(this,Object(d.a)(t).apply(this,arguments))}return Object(m.a)(t,e),Object(s.a)(t,[{key:"componentDidMount",value:function(){document.title="Predictor measurement heterogeneity affects performance of clinical prediction models";var e=document.createElement("script");e.src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML",e.async=!0,document.body.appendChild(e),(e=document.createElement("script")).src="MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});",document.body.appendChild(e)}},{key:"handleClick",value:function(e){"panel chat"!==e.target.parentNode.parentNode.parentNode.className&&"panel chat"!==e.target.parentNode.parentNode.parentNode.parentNode.className||this.forceUpdate()}},{key:"render",value:function(){return n.a.createElement("div",{className:"App"},n.a.createElement("div",{dangerouslySetInnerHTML:{__html:"<script type='text/javascript' async src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js'><\/script><script type='text/x-mathjax-config'>MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\\\(','\\\\)']]}});<\/script>"}}),n.a.createElement("div",{dangerouslySetInnerHTML:{__html:"<script type='text/x-mathjax-config'>window.onload = ()=>{document.querySelectorAll('.container a').forEach(n=>n.addEventListener('click', function(e){e.stopPropagation();}))}<\/script>"}}),n.a.createElement("div",{className:"header",style:{textAlign:"center"}},n.a.createElement("h1",{className:"poster-title"},"Measurement matters: how differences in measurement induce non-transportability of clinical prediction models"),n.a.createElement("h3",{className:"poster-authors"},"Kim Luijken, L. Wynants, M. van Smeden, D. Timmerman, B. Van Calster, E.W. Steyerberg, R.H.H. Groenwold"),n.a.createElement("h3",{className:"poster-affiliations"},"Departments of Clinical Epidemiology, Leiden University Medical Centre (LUMC), the Netherlands; Development and Regeneration, University of Leuven, Belgium; Obstetrics and Gynaecology, University Hospital Gasthuisberg, K.U.Leuven, Belgium; Biomedical Data Sciences, LUMC, the Netherlands; Public Health, Erasmus MC, Rotterdam, the Netherlands."),n.a.createElement("h4",{className:""},n.a.createElement("mark",null,"Tap on the bubbles to expand explanation.")),n.a.createElement("h4",{className:" "},"This poster accompanies our ",n.a.createElement("a",{href:"#Luijken2019"},"methods paper")," and empirical illustration (forthcoming) on the impact of predictor measurement heterogeneity.")),n.a.createElement("div",{onClick:this.handleClick.bind(this),className:"container"},n.a.createElement("div",{className:"panel chat",style:{gridArea:"chat"}},n.a.createElement(u,null,n.a.createElement("div",null,"Performance of a clinical prediction model in a test/validation setting is usually worse than the setting in which it was derived. Do you know why that is?"),n.a.createElement("exp",null," Prediction models need to provide accurate and reliable predictions for patients that were not part of the dataset in which the model was derived (i.e., derivation set). The ability of a prediction model to predict in future patients (i.e., out-of-sample) can be evaluated in an external validation study. Out-of-sample predictive performance is in general expected to be lower than performance estimated at derivation. See the ",n.a.createElement("a",{href:"#Steyerberg2008"},"book")," 'Clinical Prediction Models' by E.W. Steyerberg (2008) for a thorough introduction on clinical prediction modelling or an overview of important considerations ",n.a.createElement("a",{target:"_blank",href:"https://doi.org/10.1111/1471-0528.14170"},"here"),".")),n.a.createElement(h,{user:"Clinical prediction research"},n.a.createElement("div",null,'Because of overfitting and differences in patient characteristics ("case-mix") between derivation and validation settings.'),n.a.createElement("exp",null,n.a.createElement("p",null,"While out-of-sample predictive performance is in general expected to be lower than performance estimated at derivation, large discrepancies are often contributed to suboptimal modeling stategies in the derivation of the model and differences between patient characteristics in derivation and validation samples. An overview of important modelling aspects and discrepancies between derivation and validation settings can be found in the ",n.a.createElement("a",{target:"_blank",href:"https://doi.org/10.1136/bmj.g7594 "},"TRIPOD statement"),"."))),n.a.createElement(u,null,"Absolutely! And what do you mean by case-mix differences exactly?"),n.a.createElement(h,{user:" "},n.a.createElement("div",null,"A case-mix differs across settings when disease severity of patients varies or when different types of healthcare settings are studied, for instance."),n.a.createElement("exp",null,"Generally speaking, case-mix differences are either defined in statistical terms or in substantive terms, such as clinical, behavioural and environmental characteristics. In the explanation above, substantive examples are mentioned.")),n.a.createElement(u,null,"Ok, and how can I assess the impact of case-mix differences on the performance of prediction models?"),n.a.createElement(h,{user:n.a.createElement("a",{href:"#Debray2015"},"Debray et al (2015)")},n.a.createElement("div",null,"Case-mix heterogeneity can be defined statistically."),n.a.createElement("exp",null,n.a.createElement("div",null,"We proposed to quantify the degree of relatedness between derivation and validation samples by evaluating the correspondence of the mean and spread of the linear predictor."),n.a.createElement("br",null),n.a.createElement("div",null,n.a.createElement("a",{href:"#Kundu2017"},"Kundu et al. (2017)")," show that this is directly linked to the correlation structure of the predictors in a multivariable model as well."))),n.a.createElement("br",null),n.a.createElement(h,{user:n.a.createElement("a",{href:"#Pajouheshnia2019"},"Pajouheshnia et al (2019)")},n.a.createElement("div",null,"And if we zoom in on a specific substantive explanation: differences between samples across settings of derivation and validation could rise from differences in predictor measurement strategies."),n.a.createElement("exp",null,"The magnitude or structure of the errors in predictor measurements can differ between settings of derivation and validation. When a prediction model is applied to a different setting to the one in which it was derived, its discriminative ability can decrease or increase.")),n.a.createElement(u,null,n.a.createElement("div",null,"Intriguing! Can we evaluate the impact of predictor measurements on predictive performance? Anyone experience in quantifying this?")),n.a.createElement(h,{user:n.a.createElement("span",null,n.a.createElement("a",{href:"#Khudyakov2015"},"Khudyakov et al. (2015)"),",",n.a.createElement("a",{href:"#Rosella2012"},"Rosella et al. (2012)"))},n.a.createElement("div",null,"We found that measurement error reduces the apparent performance of clinical prediction models."),n.a.createElement("exp",null," ",n.a.createElement("a",{href:"#Khudyakov2015"},"Khudyakov et al. (2015)")," compared the apparent performance of a prediction model that included one variable measured without error, along with error-free covariates, to that of a model that included a surrogate variable containing measurement error along with the error-free covariates. Apparent predictive performance was evaluated by the C-statistic, the Brier score, and the ratio of the observed to the expected number of events (OE ratio, as measure of calibration). A numerical study showed that using the error-prone covariate instead of the error-free covariate can reduce the C-statistic and increase the Brier score. The prediction model with the error-prone covariate was found to be well-calibrated, based on the OE ratio.",n.a.createElement(u,null,n.a.createElement("div",null,"To briefly explain the terminology,"),n.a.createElement("exp",null,n.a.createElement("p",null,"Apparent predictive performance: the performance of a prediction model in its derivation sample."),n.a.createElement("p",null,"C-statistic: measure of discriminatory performance. The c-statisticis a rank-order statistic that typically ranges from 0.5 (no                      discrimination) to 1 (perfect discrimination) and is equal to the area under the receiver operating characteristic (ROC) curve for a                     binary outcome."),n.a.createElement("p",null,"Brier score: measure of overall accuracy. The Brier score is a proper scoring rule that indicates the distance between predicted and                 observed outcomes."),n.a.createElement("p",null,"Calibration: measure of reliability of the prediction model, indicating the correspondence of observed event proportions and predicted                risks in a given population. More on calibration later on."),n.a.createElement(u,null,n.a.createElement("div",null,"And about the calibration,"),n.a.createElement("exp",null,"The paper by ",n.a.createElement("a",{href:"#VanCalster2016"},"Van Calster et al. (2016)")," explains that a maximum likelihood achieves this level of                      calibration by definition ",n.a.createElement("b",null,"in its derivation sample"),", in line with the finding by Khudyakov et al.")))))),n.a.createElement(u,null,"Interesting finding. Hmm, but what about predictive performance in the test/validation data?",n.a.createElement("exp",null,"Ultimately, we would like to evaluate the accuracy and reliability of predictions for patients that were not part of the derivation set.")),n.a.createElement(h,{user:""},"How could we analyze that?"),n.a.createElement(h,{user:""},"Hold on one sec: are we talking about measurement error right now? I thought we all agreed that's not important to prediction research!",n.a.createElement("exp",null,n.a.createElement("div",null,'"Generally, there is no need for the modeling of measurement error to play a role in the prediction problem. If a predictor $X$ is measured        with error and one wants to predict a response based on the error-prone version $W$ of $X$, then [...] it rarely makes any sense to worry about          measurement error. The reason for this is quite simple: $W$ is error-free as a measurement of ',n.a.createElement("i",null,"itself"),'!" (p.38, ',n.a.createElement("a",{href:"#Carroll2006"},"Carroll, 2006"),")",n.a.createElement("small",{className:"speech-bubble1-special",style:{fontSize:"0.8em",padding:"0.2em 0.5em"}},"(This is in line with our findings, yet often misinterpreted)")),n.a.createElement("br",null),n.a.createElement("exp",null,"A systematic review by ",n.a.createElement("a",{href:"#Whittle2018"},"Whittle et al. (2018)")," indeed shows that measurement error is often not discussed in clinical prediction research."))),n.a.createElement(u,null,n.a.createElement("div",null,"Indeed, measurement error is commonly thought not to be important in prediction research. We propose to think about measurement in a different way within prediction contexts. It's not necessarily the degree of error in measurements ",n.a.createElement("i",null,"within")," the research setting that is of relevance, but rather the differences in degree of measurement error ",n.a.createElement("i",null,"across")," settings.")),n.a.createElement(h,{user:""},"Ok, so how can we analyze the effect of differences in predictor measurement strategies on predictive performance in the test/validation data?"),n.a.createElement(u,{className:"result"},n.a.createElement("div",{className:"panel-heading"},n.a.createElement("h3",{className:"bubble-title"},"Using measurement error models to define heterogeneity in predictor measurements across settings")),n.a.createElement("p",null,"Measurement error models specify how an observed measurement, often denoted $W$, deviates from it's true value, often denoted $X$ (see ",n.a.createElement("a",{href:"#Carroll2006"},"Carroll, 2006")," or ",n.a.createElement("a",{href:"#Keogh2014"},"Keogh et al., 2014")," for a more elaborate specification of measurement error models). For example, think of measuring the predictor 'bodyweight'. We can imagine all patients to have a true bodyweight $X$, and that we are measuring some value that might deviate upwards or downwards when using a scale to measure bodyweight: $W$. "),n.a.createElement("div",null,"However, for prediction purposes, it is hardly ever feasible (or even undesirable) to obtain error-free measurements in clinical practice. Hence, rather than defining 'true' and 'observed' values, we apply the measurement error models to define two measurement procedures of the same variable. For example, the predictor 'bodyweight' can be measured using a scale or by asking for self-reported weight. The connection between these procedures can be formally defined using measurement error models.  One could roughly say that we specified",n.a.createElement("blockquote",null,"Measurement$_W$ = Measurement$_X$ + procedural variance,"),"were Measurement$_W$ denotes the predictor measurement procedure with the highest variance and Measurement$_X$ denotes the predictor measurement procedure with the lowest variance. We use the terms ",n.a.createElement("i",null,"preferred")," measurement for $X$ and ",n.a.createElement("i",null,"pragmatic")," measurement for $W$. Either procedure can be applied in either derivation or validation setting."),n.a.createElement(u,null,n.a.createElement("div",null,"More precisely,"),n.a.createElement("exp",null,n.a.createElement("p",null," To distinguish different measurements of the same predictor, we denote an exact measurement of the predictor (e.g. bodyweight measured on a scale) by $X$ and a pragmatic measurement (e.g. self-reported weight) by $W$. The connection between $X$ and $W$ can be formally defined using measurement error models (",n.a.createElement("a",{href:"#Keogh2014"},"Keogh et al., 2014"),"). Assuming that the relation between $X$ and $W$ is linear and additive, the association between $W$ and $X$ can be described as",n.a.createElement("p",null,"$$E(W | Y = y) = \\psi_{Y = y} + \\theta_{Y = y}E(X)  + \\epsilon_{Y = y},$$"),n.a.createElement("p",null,"$$Var(W | Y = y) = \\theta_{Y = y}^2\\sigma_X^2  + \\sigma_{\\epsilon_{Y = y}}^2,$$"),"where $\\epsilon$$_Y$",n.a.createElement("small",null,"="),"$_y$ ~ N(0,$\\sigma$$_\\epsilon$$^2$) and all parameters may depend on the value of $Y$, indicating that measurements can differ between individuals in which the outcome is observed (cases) and individuals in which the outcome is not observed (non-cases). The parameter $\\psi$ reflects the mean difference between $X$ and $W | Y$ = $y$, $\\theta$ indicates the linear association between measurement $W | Y$ = $y$ and $X$, and $\\sigma$$_\\epsilon$$^2$ reflects variance introduced by random deviations in the measurement process, where a larger $\\sigma$$_\\epsilon$$^2$ indicates that the measurement $W$ is less precise."),n.a.createElement("small",null,n.a.createElement(u,null,n.a.createElement("div",null,"So,"),n.a.createElement("exp",null,"The term ",n.a.createElement("i",null,"measurement error")," applies to situations where both an exact measurement and a pragmatic measurement of a predictor are available within a setting (e.g., the derivation set), and thus where the parameters $\\psi$, $\\theta$ and $\\sigma$$_\\epsilon$$^2$ define the degree of measurement error in $W$ with respect to $X$. The term ",n.a.createElement("i",null,"measurement heterogeneity")," refers to situations where the same predictor is measured heterogeneously across settings of derivation and validation. The most precise measurement (whether available at derivation or validation) corresponds to $X$ and the parameters $\\psi$, $\\theta$ and $\\sigma$$_\\epsilon$$^2$ define the degree of heterogeneity between $X$ and $W$.")))))),n.a.createElement(h,{user:""},"Nice framework. Before you present your results, can you give an example of predictor measurement heterogeneity?"),n.a.createElement(u,null,n.a.createElement("p",null,"Let's consider the measurement of bodyweight. A research protocol could specify that bodyweight should be obtained using a scale (and perhaps other conditions, such as standarized subject position and -clothing). However, in clinical practice, a self-reported measure of bodyweight may be obtained. When a prediction model containing the predictor 'bodyweight' is derived based on the standardized instrumental measure and validated using the self-reported measure, or vice versa, we say that this predictor is measured heterogeneously across settings."),n.a.createElement("exp",null," Another example would be the measurement of blood pressure values. A research protocol could specify that blood pressure values should be obtained by averaging three blood pressure measurements that are taken under standardized conditions. However, in clinical practice, blood pressure is often measured a single time under non-standard conditions.")),n.a.createElement(u,{id:"Measurement heterogeneity-I",className:"result"},n.a.createElement("div",{className:"panel-heading"},n.a.createElement("h3",{className:"bubble-title"},"I. Measurement heterogeneity: from precise measurement at derivation to less precise measurement at validation")),n.a.createElement("p",null,"We derived a single-predictor binary logistic prediction model using a precise predictor measurement and validated it using a less precise predictor measurement. The predictive performance at external validation is presented in ",n.a.createElement("a",{href:"#figure1"},"Figure 1")," below.")),n.a.createElement(u,{id:"figure1"},n.a.createElement("img",{src:"Overfit.PNG",alt:"Predictive performance"}),n.a.createElement("exp",null,n.a.createElement("p",null,"Fig 1. Measures of predictive performance under predictor measurement heterogeneity."),n.a.createElement("p",null,"Obtained in a derivation and validation sample of N = $1,000,000$ each."),n.a.createElement(u,null,n.a.createElement("div",null,"What is C-stat?"),n.a.createElement("exp",null,"The c-statistic; a rank-order statistic that typically ranges from 0.5 (no discrimination) to 1 (perfect discrimination) and is equal to the area under the receiver operating characteristic (ROC) curve for a binary outcome.")),n.a.createElement(u,null,n.a.createElement("div",null,"What are intercept and slope?"),n.a.createElement("exp",null,"These are measures to evaluate calibration of the prediction model. In logistic regression, calibration can be determined using a re-calibration model, where the observed outcomes in validation data, $y_V$, are regressed on a linear predictor (lp). This linear predictor is obtained by combining the regression coefficients estimated from the derivation data, $\\alpha_D$ and $\\beta_D$, with the predictor values in the validation data, $x_V$. The recalibration model is defined as",n.a.createElement("p",null,"$$\\text{logit}(y_V) = a + b\\times\\text{lp},$$"),"where $lp = \\alpha_D + \\beta_Dx_V$ and $b$ represents the calibration slope. A calibration slope $b = 1$ indicates  perfect calibration. A calibration slope b less than $1$ indicates that predicted probabilities are too extreme compared to observed probabilities, which is often found in situations of 'statistical overfitting'. A calibration slope b greater than $1$ indicates that the provided predicted probabilities are too close to the outcome incidence, also referred to as 'statistical underfitting'. Additional to the calibration slope, we evaluated the difference between the average observed event rate and the mean predicted event rate (i.e. calibration-in-the-large, which can be computed as the intercept of the recalibration model while using an offset for the linear predictor, i.e., $a | b = 1$ ). See ",n.a.createElement("a",{href:"#VanCalster2016"},"Van Calster et al. (2016)")," for a more elaborate discussion on calibration.")),n.a.createElement(u,null,n.a.createElement("div",null,"What is Brier?"),n.a.createElement("exp",null,"The Brier score, a measure of overall accuracy. The Brier score is a proper scoring rule that indicates the distance between predicted and observed outcomes.")))),n.a.createElement(h,null,"Oh, and what does this predictive performance imply?"),n.a.createElement(u,null,n.a.createElement("p",null,"When predictor measurements are more precise at derivation compared to validation, model discrimination and accuracy at validation deteriorate, and the provided predicted probabilities are too extreme, similar to when a model is overfitted with respect to the derivation data. The predictions are thus no longer correct.")),n.a.createElement(h,null,"And what do you mean exactly with precise at derivation and less precise at validation?"),n.a.createElement(u,null,n.a.createElement("p",null," Think of the bodyweight example again. Most likely, the standardized instrumental measurement of bodyweight will have a higher precision than the self-reported measurement. So, following this example, the standardized instrumental measurement of bodyweight was used at model derivation and the self-reported measurement of bodyweight was used at validation.")),n.a.createElement(u,{id:"Measurement heterogeneity-II",className:"result"},n.a.createElement("div",{className:"panel-heading"},n.a.createElement("h3",{className:"bubble-title"},"II. Measurement heterogeneity: from less precise measurement at derivation to precise measurement at validation"))),n.a.createElement(u,null,n.a.createElement("img",{src:"Underfit.PNG",alt:"Predictive performance"}),n.a.createElement("exp",null,n.a.createElement("p",null,"Fig 2. Measures of predictive performance under predictor measurement heterogeneity."),n.a.createElement("p",null,"The measures of predictive performance are obtained the same way as in ",n.a.createElement("a",{href:"#figure1"},"Figure 1"),"."))),n.a.createElement(u,null,n.a.createElement("p",null,"When predictor measurements are less precise at derivation compared to validation, discrimination and accuracy at validation tend to improve, but the provided predicted probabilities are too close to the outcome prevalence, similar to statistical underfitting.")),n.a.createElement(u,{className:"result"},n.a.createElement("div",{className:"panel-heading"},n.a.createElement("h3",{className:"bubble-title"},"Main result simulations")),n.a.createElement("img",{src:"Overview.PNG",alt:"Predictive performance different scenarios"}),n.a.createElement("exp",null,n.a.createElement("p",null,"Fig 3. "),n.a.createElement("div",null,"Left panel: model derived on precise predictor measurements and validated on less precise predictor measurements. "),n.a.createElement("div",null,"Middle panel: model derived and validated on predictor measurements with equal precision. "),n.a.createElement("div",null,"Right panel: model derived on less precise predictor measurements and validated on precise predictor measurements. "),n.a.createElement(u,null,n.a.createElement("div",null,"So what is the difference in predictive performance under predictor measurement ",n.a.createElement("i",null,"homogeneity")," versus under predictor measurement ",n.a.createElement("i",null,"heterogeneity"),"?"),n.a.createElement("exp",null,"The middle panel shows predictive performance under predictor measurement homogeneity across settings. Only in this case, the calibration intercept and slope indicate good calibration. When predictor measurements at validation are different from derivation (measurement heterogeneity), the model is miscalibrated.")),n.a.createElement(u,null,n.a.createElement("div",null,"More info"),n.a.createElement("exp",null,"If you would like to further wrap your head around these findings, please visit our Shiny app ",n.a.createElement("a",{target:"_blank",href:"https://kluijken.shinyapps.io/MH_Predict_Examples/"},"here")," to inspect the implications of predictor measurement heterogeneity under various conditions.")))),n.a.createElement(h,null,n.a.createElement("div",null,"Do you have a clinical example to clarify a bit further what you mean?")),n.a.createElement(u,null,n.a.createElement("p",null,"Sure! We wondered about the impact of real-world predictor measurement heterogeneity as well, since real-world scenarios are more complex than our simulations due to the greater variation in measurement structures and the multivariable context. We evaluated this for 10 empirical examples, of which we discuss 1 here."),n.a.createElement(u,null,n.a.createElement("div",null,"What dataset did we evaluate?"),n.a.createElement("exp",null,"The International Ovarian Tumour Analysis (IOTA) dataset includes clinical and ultrasound information on 5,914 non-pregnant women with at least one persistent adnexal mass (",n.a.createElement("a",{href:"#Timmerman2000"},"Timmerman et al."),"). IOTA data collection has occurred in several phases since 1999. Here, data from IOTA phases I to III are used.")),n.a.createElement(u,null,n.a.createElement("div",null,"What was the prediction model?"),n.a.createElement("exp",null,"We evaluated a logistic regression model that estimates the probability of presence of ovarian mass malignancy from the predictors age, the presence of ascites, the presence of blood flow within a solid papillary projection, the maximal diameter of the largest solid component, the presence of irregular cyst walls and the presence of acoustic shadows. It was developed and internally validated in IOTA phase I data and has been externally validated several times.")),n.a.createElement(u,null,n.a.createElement("div",null,"How was the predictor measurement heterogeneity defined?"),n.a.createElement("exp",null,"We defined predictor measurement heterogeneity for various predictors in the model. Here, we discuss the predictor 'Diameter of the solid component of the ovarian tumor'. This predictor was measured according to two strategies: (i) the diameter was truncated at 50mm or (ii) the original value of the diameter was used. We refer to the truncated diameter as the ",n.a.createElement("i",null,"preferred")," measurement, and to the non-truncated diameter as the ",n.a.createElement("i",null,"pragmatic")," measurement. The truncated diameter is preferred because this was found to be the most informative predictor in previous studies in the IOTA data.",n.a.createElement(h,null,"Why would anyone use a measure of the diameter different from the one the model was derived from? "),n.a.createElement("exp",null,"This could be motivated by various reasons. For one, the model could be implied in a clinical setting where a physician is unaware that the values should be truncated. Additionally, a dataset may simply not contain the particular measurement. When this dataset is readily available, researchers may want to use it for model validation despite the measurement heterogeneity.")))),n.a.createElement(u,{className:"result"},n.a.createElement("div",{className:"panel-heading"},n.a.createElement("h3",{className:"bubble-title"},"Results empirical example")),n.a.createElement("div",null,"When the preferred predictor measurement (truncated diameter) was substituted by the pragmatic predictor measurement (non-truncated diameter), the predictive performance measures at validation were as follows:"),n.a.createElement("p",null,"Calibration-in-the-large coefficient: -0.437 (-0.504; -0.371), indicating that the event fraction was underestimated."),n.a.createElement("p",null,"Calibration slope: 0.705 (0.654; 0.750), indicating the model was overdispersed."),n.a.createElement("p",null," The c-statistic and Brier score changed minimally, by -0.009 (-0.010;-0.007) and -0.009 (-0.031; -0.017), respectively. ",n.a.createElement("small",null,"(Note that we have scaled the Brier score here by its maximum score.)")),n.a.createElement("exp",null,n.a.createElement("div",null,"When the preferred predictor measurement (truncated diameter) was substituted by the pragmatic predictor measurement (non-truncated diameter), the predictive performance measures at validation were as follows:"),n.a.createElement("p",null,"Calibration-in-the-large coefficient: 0.217 (0.115; 0.323), indicating that the event fraction was overestimated."),n.a.createElement("p",null,"Calibration slope: 1.081 (1.025; 1.139), indicating the model was underdispersed."),n.a.createElement("p",null," The c-statistic and Brier score changed minimally, by 0.0002 (-0.0007; 0.0010) and -0.018 (-0.024; -0.013), respectively."),n.a.createElement(u,null,n.a.createElement("div",null,"Over all 10 scenarios, "),n.a.createElement("exp",null,n.a.createElement("p",null,"the predictive performance at validation was quite different from performance at derivation, mainly in terms of calibration. The calibration-in-the-large coefficient (intercept) ranged from -2.3 to 1.6 (0 for good calibration) and the calibration slope ranged from 0.7 to 1.1 (1 for good calibration)."))))),n.a.createElement(h,{user:""},n.a.createElement("p",null,"Ok, that seems to be quite impactful for the validity and reliability of predictions at validation. How can I correct for predictor measurement heterogeneity?")),n.a.createElement(u,null,n.a.createElement("div",null,"To be honest, although correction for measurement error is possible in theory, we expect that the applicability of these methods to correct for measurement heterogeneity in prediction studies will be limited. This is not only due to the fact that individual patient data of both the derivation and validation set are required, but furthermore because it is infeasible to disentangle measurement parameters from other characteristics of the data. The main contribution of the taxonomy of measurement error models rises from its aptitude to conceptualize measurement heterogeneity across settings in pragmatic terms."),"      ",n.a.createElement("exp",null,n.a.createElement("p",null,"Additional to measurement correction methods, one could think about methods that address the miscalibration of the model. For example, it is recommended to apply shrinkage methods to prevent model overfitting. We think about measurement heterogeneity as a separate issue. Under different scenarios of measurement heterogeneity, shrinkage could both improve and diminish the calibration of the model at validation. Rather than correcting for measurement heterogeneity in a statistical way, measurement procedures should be considered in the design phase of as study (i.e., at data collection)."),n.a.createElement("p",null,"Note that this does NOT imply we recommend against shrinkage, but the motivation to apply it should come from modelling considerations rather than data characteristics."))),n.a.createElement(h,{user:n.a.createElement("a",{href:"#Steyerberg2018"},"Steyerberg et al (2018)")},n.a.createElement("div",null,"Don't forget about modelling issues.")),n.a.createElement(u,null,"Absolutely, all recommendations are additional to modelling strategies and sample size considerations."),n.a.createElement(h,null,"What do you recommend for epidemiologic prediction studies?"),n.a.createElement(u,{id:"Recommendations",className:"result"},n.a.createElement("div",{className:"panel-heading"},n.a.createElement("h3",{className:"bubble-title"},"Recommendations")),n.a.createElement("p",null,"The following implications follow from our work:"),n.a.createElement("p",null,"Ideally, prediction models are derived using predictor measurements that resemble measurement procedures in the intended setting of application. A mismatch is likely to result in miscalibration of the model."),n.a.createElement("p",null,"Researchers should bear in mind the implications of using a 'readily available dataset' for model derivation or validation as data quality directly affects estimates of predictive performance of the model."),n.a.createElement("p",null,"Descriptions of measurement procedures at model derivation are essential for proper external validation of the model. Likewise, validation studies ideally contain descriptions of deviations from measurements used at derivation, as these may introduce discrepancies in predictive performance.")),n.a.createElement(h,{user:n.a.createElement("a",{href:"#Collins2015"},"Collins et al (2015)")},n.a.createElement("div",null,"Actually, you quantified ",n.a.createElement("a",{target:"_blank",href:"https://www.tripod-statement.org/TRIPOD/TRIPOD-Checklists/TRIPOD-Checklist-Prediction-Model-Development-and-Validation"},"item 7")," in our TRIPOD checklist.")),n.a.createElement(h,{user:n.a.createElement("a",{href:"#Wynants2013"},"Wynants et al (2013)")},n.a.createElement("div",null,"In case of multicenter prediction studies, it might also be wise to quantify the measurement variability ",n.a.createElement("i",null,"within")," the overall study dataset. You can use the residual intraclass correlation for that!")),n.a.createElement(h,{user:n.a.createElement("a",{href:"#Whittle2018"},"Whittle et al (2018)")},n.a.createElement("div",null,"More attention for 'measurement error' and measurement strategies in prediction research would be a great development!")),n.a.createElement(u,null,"Thanks everyone! Great to go over these implications of differences in predictor measurement procedures. A prediction model really isn't only about the algorithm relating predictors to the outcome, but also depends on the procedures by which model input is measured, i.e. qualitative differences in data collection."),n.a.createElement(u,null,n.a.createElement("p",null,"We hope this info shed a different light on the importance of measurement in prediction research. We highlight heterogeneity in predictor measurement procedures ",n.a.createElement("i",null,"across")," settings as an important driver of unanticipated predictive performance at external validation. Preventing measurement heterogeneity at the design phase of a prediction study, both in development and validation studies, facilitates interpretation of predictive performance and benefits the transportability of the prediction model.")),n.a.createElement(u,null,n.a.createElement("p",null,"Interested in learning more about this topic?"),n.a.createElement("p",null,"We refer to:"),n.a.createElement("p",null," This ",n.a.createElement("a",{target:"_blank",href:"https://doi.org/10.1002/sim.8183"},"paper")," for the simulation studies, with a shiny app ",n.a.createElement("a",{target:"_blank",href:"https://kluijken.shinyapps.io/MH_Predict_Examples/"},"here")," to explore the results further."),n.a.createElement("p",null," This paper [here] for the empirical studies. "),n.a.createElement("p",null," Please contact k.luijken@lumc.nl if you have any questions or suggestions.")),n.a.createElement(h,{style:{fontSize:"0.8em"}},n.a.createElement("p",null,n.a.createElement("b",null,"References:")),n.a.createElement("p",null,n.a.createElement("a",{id:"Luijken2019",href:"https://doi.org/10.1002/sim.8183",target:"_blank"},"Luijken et al (2019). Impact of predictor measurement heterogeneity across settings on performance of prediction models: a measurement error perspective.  Published in Stat Med 2019")),n.a.createElement("p",null,n.a.createElement("a",{id:"Steyerberg2008",target:"_blank"},"Steyerberg (2008). Clinical prediction models: a practical approach to development, validation, and updating. Published in 2008")),n.a.createElement("p",null,n.a.createElement("a",{id:"Collins2015",href:"https://doi.org/10.1136/bmj.g7594",target:"_blank"},"Collins et al (2015). Transparent reporting of a multivariable prediction model for individual prognosis or diagnosis (TRIPOD): the TRIPOD statement.  Published in BMJ 2015")),n.a.createElement("p",null,n.a.createElement("a",{id:"Debray2015",href:"https://doi.org/10.1016/j.jclinepi.2014.06.018",target:"_blank"},"Debray et al (2015). A new framework to enhance the interpretation of external validation studies of clinical prediction models.  Published in JCE 2015")),n.a.createElement("p",null,n.a.createElement("a",{id:"Kundu2017",href:"https://doi.org/10.1186/s12874-017-0345-1",target:"_blank"},"Kundu et al (2017). Impact of correlation of predictors on discrimination of risk models in development and external populations.  Published in BMC MRM 2017")),n.a.createElement("p",null,n.a.createElement("a",{id:"Pajouheshnia2019",href:"https://doi.org/10.1016/j.jclinepi.2018.09.001",target:"_blank"},"Pajouheshnia et al (2019). How variation in predictor measurement affects the discriminative ability and transportability of a prediction model.  Published in JCE 2019")),n.a.createElement("p",null,n.a.createElement("a",{id:"Khudyakov2015",href:"https://doi.org/10.1002/sim.6498",target:"_blank"},"Khudyakov et al (2015). The impact of covariate measurement error on risk prediction.  Published in Stat Med 2015")),n.a.createElement("p",null,n.a.createElement("a",{id:"Rosella2012",href:"https://doi.org/10.1186/1478-7954-10-20",target:"_blank"},"Rosella et al (2012). The influence of measurement error on calibration, discrimination, and overall estimation of a risk prediction model.  Published in BMC PHM 2012")),n.a.createElement("p",null,n.a.createElement("a",{id:"VanCalster2016",href:"https://doi.org/10.1016/j.jclinepi.2015.12.005",target:"_blank"},"Van Calster et al (2016). A calibration hierarchy for risk models was defined: from utopia to empirical data.  Published in JCE 2016")),n.a.createElement("p",null,n.a.createElement("a",{id:"Carroll2006",target:"_blank"},"Carroll (2006). Measurement Error in Nonlinear Models: A Modern Perspective, Second Edition. Published in 2006")),n.a.createElement("p",null,n.a.createElement("a",{id:"Whittle2018",href:"https://doi.org/10.1016/j.jclinepi.2018.05.008",target:"_blank"},"Whittle et al (2018). Measurement error and timing of predictor values for multivariable risk prediction models are poorly reported. Published in JCE 2018")),n.a.createElement("p",null,n.a.createElement("a",{id:"Keogh2014",href:"https://doi.org/10.1002/sim.6095",target:"_blank"},"Keogh et al (2014). A toolkit for measurement error correction, with a focus on nutritional epidemiology. Published in Stat Med 2014")),n.a.createElement("p",null,n.a.createElement("a",{id:"Timmerman2000",href:"https://doi.org/10.1046/j.1469-0705.2000.00287.x",target:"_blank"},"Timmerman et al (2000). Terms, definitions and measurements to describe the sonographic features of adnexal tumors: a consensus opinion from the International Ovarian Tumor Analysis (IOTA) group. Published in Ultrasound in Obs and Gyn 2000")),n.a.createElement("p",null,n.a.createElement("a",{id:"Steyerberg2018",href:"https://doi.org/10.1016/j.jclinepi.2017.11.013",target:"_blank"},"Steyerberg et al (2018). Poor performance of clinical prediction models: the harm of commonly applied methods.  Published in JCE 2018")),n.a.createElement("p",null,n.a.createElement("a",{id:"Wynants2013",href:"https://doi.org/10.1186/1471-2288-13-128",target:"_blank"},"Wynants et al (2013). Screening for data clustering in multicenter studies: the residual intraclass correlation.  Published in BMC MRM 2013"))))))}}]),t}(r.Component));Boolean("localhost"===window.location.hostname||"[::1]"===window.location.hostname||window.location.hostname.match(/^127(?:\.(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)){3}$/));l.a.render(n.a.createElement(p,null),document.getElementById("root")),"serviceWorker"in navigator&&navigator.serviceWorker.ready.then(function(e){e.unregister()})},8:function(e,t,a){e.exports=a(16)}},[[8,1,2]]]);
//# sourceMappingURL=main.dc20598e.chunk.js.map